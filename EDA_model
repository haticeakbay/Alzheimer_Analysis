https://colab.research.google.com/drive/1AmQgddy6jl866rGHZqoVSBHBMiQzaTno?usp=sharing


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,roc_auc_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, cross_val_score, validation_curve
from sklearn.model_selection import GridSearchCV, cross_validate
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.neighbors import LocalOutlierFactor

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
!pip install catboost
from catboost import CatBoostClassifier

pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.max_rows',None)
pd.set_option('display.float_format', lambda x:'%.3f'%x)

df=pd.read_csv('/content/alzheimer.csv') #uploaded file to colab but you can reach out to dataset via this link https://www.kaggle.com/datasets/brsdincer/alzheimer-features 
df.head()

#During the overview analysis, we examine the general characteristics and properties of the dataset.

def check_df(dataframe, head=5):
    print("############################# Shape #############################")
    print(dataframe.shape)
    print("\n")
    print("############################# Dtype #############################")
    print(dataframe.dtypes)
    print("\n")
    print("############################# Head #############################")
    print(dataframe.head(head))
    print("\n")
    print("############################# Tail #############################")
    print(dataframe.tail(head))
    print("\n")
    print("############################# NA #############################")
    print(dataframe.isnull().sum())
    print("\n")
    print("############################# Quantiles #############################")
    print(dataframe.describe([0, 0.05, 0.5, 0.95, 0.99, 1]).T)
    print("\n")

check_df(df)

#Capturing Numerical and Categorical Variables (Resolving Type Errors if Present)

def grab_col_names(dataframe, cat_th=10, car_th=20):

    # cat_cols, cat_but_car
    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == "O"]
    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and
                   dataframe[col].dtypes != "O"]
    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and
                   dataframe[col].dtypes == "O"]

    cat_cols = cat_cols + num_but_cat
    cat_cols = [col for col in cat_cols if col not in cat_but_car]

    #num_cols
    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != "O"]
    num_cols = [col for col in num_cols if col not in num_but_cat]

    print(f"Observation: {dataframe.shape[0]}")
    print(f"Variables: {dataframe.shape[1]}")
    print(f"cat_cols: {len(cat_cols)}")
    print(f"num_cols: {len(num_cols)}")
    print(f"cat_but_car: {len(cat_but_car)}")
    print(f"num_but_cat: {len(num_but_cat)}")

    return cat_cols, num_cols, cat_but_car

cat_cols, num_cols, cat_but_car = grab_col_names(df)

cat_cols

num_cols

def cat_summary(dataframe, col_name, plot = False):
    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),
                        "Ratio":100 * dataframe[col_name].value_counts() / len(dataframe)}))
    print("#############################################################")
    print("\n")

    if plot:
        sns.countplot(x=dataframe[col_name], data=dataframe)
        plt.show(block=True)

for col in cat_cols:
    cat_summary(df,col)

def num_summary(dataframe, numeric_col, plot=False):
    quantiles = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
    print(dataframe[numeric_col].describe(quantiles).T)

    if plot:
        dataframe[numeric_col].hist()
        plt.xlabel(numeric_col)
        plt.title(numeric_col)
        plt.interactive(False)
        plt.show(block=True)

for col in num_cols:
    num_summary(df, col, plot="True")

def target_summary_with_cat(dataframe, target, categorical_col):
    print(categorical_col)
    print(pd.DataFrame({"TARGET_MEAN":dataframe.groupby(categorical_col)[target].mean(),
                        "Count":dataframe.groupby(categorical_col)[target].count(),
                        "Ratio":100* dataframe[categorical_col].value_counts()/ dataframe.shape[0]}))

for col in cat_cols:
    target_summary_with_cat(df, "EDUC", col)
    print("\n")

def target_summary_with_num(dataframe, target, numerical_col):
    print(dataframe.groupby(target).agg({numerical_col:"mean"}))

for col in num_cols:
    target_summary_with_num(df, "Group", col)
    print("\n")

corr = df[num_cols].corr()

sns.set(rc = {"figure.figsize":(12,12)})
ax = sns.heatmap(corr, fmt=".2f", annot=True, cmap="RdBu") # korelasyonu gözlemleyebilmek için ısı haritası oluşturduk
ax.set_yticklabels(ax.get_yticklabels(), rotation=0)
plt.interactive(False)
plt.show()

def outlier_thresholds(dataframe, col_name, q1=0.05, q3=0.95):
    quartile1=dataframe[col_name].quantile(q1)
    quartile3=dataframe[col_name].quantile(q3)
    interquantile_range= quartile3 - quartile1
    up_limit = quartile3 + 1.5 * interquantile_range
    low_limit = quartile1 - 1.5 * interquantile_range
    return low_limit, up_limit

def check_outlier(dataframe, col_name):
    low_limit, up_limit = outlier_thresholds(dataframe, col_name)
    if dataframe[(dataframe[col_name] < low_limit) | (dataframe[col_name] > up_limit)].any(axis=None):
        return True
    else:
        return False

for col in num_cols:
    if col != "Group":
        print(col, check_outlier(df,col))

def replace_with_threshold(dataframe, variable):
    low_limit, up_limit = outlier_thresholds(dataframe, variable)
    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit
    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit

for col in num_cols:
    if col != "Group":
        print(col, check_outlier(df,col))

def missing_value_analysis(dataframe):
    print("Is there any missing data?")
    print(dataframe.isnull().values.any())
    print("##############################")
    print("\n")

    print("How much missing data is there in the variables?")
    print(dataframe.isnull().sum())
    print("##############################")
    print("\n")

    print("How many complete values are there in the variables?")
    print(dataframe.notnull().sum())
    print("##############################")
    print("\n")

    print("Total number of missing values in the dataset:")
    print(dataframe.isnull().sum().sum())
    print("##############################")
    print("\n")

missing_value_analysis(df)

def missing_values_table(dataframe, na_name=False):
    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]
    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)
    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)
    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])
    print(missing_df, end="\n")
    if na_name:
        return na_columns

na_columns = missing_values_table(df, na_name=True)

plt.figure(figsize=(6, 8))
plt.title('Missing Values')
ax = sns.heatmap(df.isna().sum().to_frame(), annot=True, fmt='d', cmap='Blues')
plt.show()

def quick_missing_imp(data, num_method="median", cat_length=20, target="Group"):
    variables_with_na = [col for col in data.columns if data[col].isnull().sum() > 0]  # Lists the variables with missing values

    temp_target = data[target]

    print("# BEFORE")
    print(data[variables_with_na].isnull().sum(), "\n\n")  # Number of missing values in variables before the application

    # If the variable is object type and has a number of unique values less than or equal to cat_length, fill the missing values with mode
    data = data.apply(lambda x: x.fillna(x.mode()[0]) if (x.dtype == "O" and len(x.unique()) <= cat_length) else x, axis=0)

    # If num_method is 'mean', fill the missing values of non-object type variables with the mean
    if num_method == "mean":
        data = data.apply(lambda x: x.fillna(x.mean()) if x.dtype != "O" else x, axis=0)
    # If num_method is 'median', fill the missing values of non-object type variables with the median
    elif num_method == "median":
        data = data.apply(lambda x: x.fillna(x.median()) if x.dtype != "O" else x, axis=0)

    data[target] = temp_target

    print("# AFTER \n Imputation method is 'MODE' for categorical variables!")
    print(" Imputation method is '" + num_method.upper() + "' for numeric variables! \n")
    print(data[variables_with_na].isnull().sum(), "\n\n")

    return data

df = quick_missing_imp(df, num_method="median", cat_length=17)

dff = df.copy()

cat_cols, cat_but_car, num_cols = grab_col_names(dff)

def label_encoder(dataframe, binary_col, drop_first=True):
    labelencoder = LabelEncoder()
    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])
    return dataframe

binary_cols = [col for col in dff.columns if dff[col].dtype not in [int, float] and dff[col].nunique() == 2]
print(binary_cols)

for col in binary_cols:
    label_encoder(dff, col)

dff.head()

def one_hot_encoder(dataframe, categorical_cols, drop_first=True):
    dataframe = pd.get_dummies(df, columns=categorical_cols, drop_first=drop_first)
    return dataframe

dff = one_hot_encoder(dff, cat_cols, drop_first=True)

dff.shape

dff.head()

df_hold_out = dff.copy()

y = df_hold_out["Group_Demented"].astype(int)
X = df_hold_out.drop(["Group_Demented"], axis=1)

X_train, X_test, y_train,  y_test = train_test_split(X, y, test_size = 0.2, random_state = 17)

models = [("LR", LogisticRegression()),
          ("KNN", KNeighborsClassifier()),
          ("CART", DecisionTreeClassifier()),
          ("RF", RandomForestClassifier()),
          ("GBM", GradientBoostingClassifier()),
          ("CatBoost", CatBoostClassifier(iterations = 500, verbose=False))
         ]

group_demented = ['True', 'False']

for name, classifier in models:

    #warnings.filterwarnings("ignore")

    model = classifier.fit(X_train,y_train)
    y_pred = model.predict(X_test)
    y_pred_train = model.predict(X_train)

    # The confusion matrix allows us to examine the correct and incorrect prediction results for each class in the models.
    # This provides a detailed analysis opportunity for accuracy evaluation.

    # Confusion matrix for the test set provides the following results
    print(f"Performance Metrics for {name} on Test and Train Sets")
    cm_test = confusion_matrix(y_test, y_pred)

    group_demented = np.unique(y_test)
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', xticklabels=group_demented, yticklabels=group_demented)

    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title(f'Confusion Matrix for Test Set ({name})')
    plt.show()
    print("\n")

    # Classification report provides f1, precision, recall, and accuracy values for each class in all models.
    # It is used for a detailed analysis of performance metrics in classification models.

    cr_test = classification_report(y_test, y_pred)
    print(f"Classification Report for Test Set ({name}) :\n{cr_test}")
    print("\n")

    # Confusion matrix results for the training set

    cm_train = confusion_matrix(y_train, y_pred_train)

    group_demented = np.unique(y_test)
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', xticklabels=group_demented, yticklabels=group_demented)

    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title(f'Confusion Matrix for Train Set ({name})')
    plt.show()

    cr_train = classification_report(y_train, y_pred_train)
    print(f"Classification Report for Train Set ({name}) :\n{cr_train}")
    print("-----------------------------------------------------------------------")
    print("\n")

df_cross_validation = dff.copy()

for name, classifier in models:

    #warnings.filterwarnings("ignore")

    # Perform k-fold cross-validation using the cross_val_score function.
    scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']
    cv_results = cross_validate(classifier, X, y, cv=5, scoring=scoring)

    # Display the results.
    print(f"---------------------- {name} Model ----------------------")

    print("Accuracy:", cv_results['test_accuracy'])
    print("Precision:", cv_results['test_precision_macro'])
    print("Recall:", cv_results['test_recall_macro'])
    print("F1-score:", cv_results['test_f1_macro'])
    print("..")
    print("Average Accuracy:", cv_results['test_accuracy'].mean().round(2))
    print("Average Precision:", cv_results['test_precision_macro'].mean().round(2))
    print("Average Recall:", cv_results['test_recall_macro'].mean().round(2))
    print("Average F1-score:", cv_results['test_f1_macro'].mean().round(2))
    print("\n")

df['MMSE_significant'] = df['MMSE'] >= 25
df['MMSE_mild'] = (df['MMSE'] >= 20) & (df['MMSE'] < 25)
df['MMSE_moderate'] = (df['MMSE'] >= 10) & (df['MMSE'] < 20)
df['MMSE_severe'] = (df['MMSE'] >= 0) & (df['MMSE'] < 10)

'''Ratings are assigned on a 0–5 point scale, (0 = absent; 0.5 = questionable; 1= present, but mild; 2 = moderate; 3 = severe; 4 = profound; 5 = terminal).'''
df['CDR_absent'] = df['CDR'] == 0
df['CDR_questionable'] = df['CDR'] == 0.5
df['CDR_presentmild'] = df['CDR'] == 1
df['CDR_moderate'] = df['CDR'] == 2
df['CDR_severe'] = df['CDR'] == 3
df['CDR_profound'] = df['CDR'] == 4
df['CDR_terminal'] = df['CDR'] == 5

df.loc[(df['M/F'] == "M") & (df['Age'] <= 65), 'NEW_SEX_CAT'] = 'undersixtyfivemale'           #60-98  75-84 85 over
df.loc[(df['M/F'] == "F") & (df['Age'] <= 65), 'NEW_SEX_CAT'] = 'undersixtyfivefemale'
df.loc[(df['M/F'] == "M") & (df['Age'] > 65) & (df['Age'] <= 74), 'NEW_SEX_CAT'] = 'underseventyfourmale'
df.loc[(df['M/F'] == "F") & (df['Age'] > 65) & (df['Age'] <= 74), 'NEW_SEX_CAT'] = 'underseventyfourfemale'
df.loc[(df['M/F'] == "M") & (df['Age'] > 74) & (df['Age'] <= 85), 'NEW_SEX_CAT'] = 'undereightyfivemale'
df.loc[(df['M/F'] == "F") & (df['Age'] > 74) & (df['Age'] <= 85), 'NEW_SEX_CAT'] = 'undereightyfivefemale'
df.loc[(df['M/F'] == "M") & (df['Age'] > 85), 'NEW_SEX_CAT'] = 'oldereightyfivemale'
df.loc[(df['M/F'] == "F") & (df['Age'] > 85), 'NEW_SEX_CAT'] = 'oldereightyfivefemale'

df.head()

cat_cols, num_cols, cat_but_car = grab_col_names(df)

cat_cols = [col for col in cat_cols if col not in ["Group"]]
cat_cols

df = one_hot_encoder(df, cat_cols, drop_first=True)

df.head()

df_hold_out_final = df.copy()

y = df_hold_out["Group_Demented"].astype(int)
X = df_hold_out.drop(["Group_Demented"], axis=1)

X_train, X_test, y_train,  y_test = train_test_split(X, y, test_size = 0.2, random_state = 17)

models = [("LR", LogisticRegression()),
          ("KNN", KNeighborsClassifier()),
          ("CART", DecisionTreeClassifier()),
          ("RF", RandomForestClassifier()),
          ("GBM", GradientBoostingClassifier()),
          ("CatBoost", CatBoostClassifier(iterations = 100, verbose=False))
         ]

for name, classifier in models:

    #warnings.filterwarnings("ignore")

    model = classifier.fit(X_train,y_train)
    y_pred = model.predict(X_test)
    y_pred_train = model.predict(X_train)

    # The confusion matrix allows us to examine the correct and incorrect prediction results for each class in the models.
    # This provides a detailed analysis opportunity for accuracy evaluation.

    # Confusion matrix for the test set provides the following results
    print(f"Performance Metrics for {name} on Test and Train Sets")
    cm_test = confusion_matrix(y_test, y_pred)

    group_demented = np.unique(y_test)
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', xticklabels=group_demented, yticklabels=group_demented)

    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title(f'Confusion Matrix for Test Set ({name})')
    plt.show()
    print("\n")

    # Classification report provides f1, precision, recall, and accuracy values for each class in all models.
    # It is used for a detailed analysis of performance metrics in classification models.

    cr_test = classification_report(y_test, y_pred)
    print(f"Classification Report for Test Set ({name}) :\n{cr_test}")
    print("\n")

    # Confusion matrix results for the training set

    cm_train = confusion_matrix(y_train, y_pred_train)

    group_demented = np.unique(y_test)
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', xticklabels=group_demented, yticklabels=group_demented)

    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title(f'Confusion Matrix for Train Set ({name})')
    plt.show()

    cr_train = classification_report(y_train, y_pred_train)
    print(f"Classification Report for Train Set ({name}) :\n{cr_train}")
    print("-----------------------------------------------------------------------")
    print("\n")

df_cross_validation_final = df.copy()

for name, classifier in models:

    #warnings.filterwarnings("ignore")

    # Performing k-fold cross-validation using the cross_val_score function
    scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']
    cv_results = cross_validate(classifier, X, y, cv=5, scoring=scoring)

    # To display the results
    print(f"---------------------- {name} Model ----------------------")

    print("Accuracy:", cv_results['test_accuracy'])
    print("Precision:", cv_results['test_precision_macro'])
    print("Recall:", cv_results['test_recall_macro'])
    print("F1-score:", cv_results['test_f1_macro'])
    print("..")
    print("Average Accuracy:", cv_results['test_accuracy'].mean().round(2))
    print("Average Precision:", cv_results['test_precision_macro'].mean().round(2))
    print("Average Recall:", cv_results['test_recall_macro'].mean().round(2))
    print("Average F1-score:", cv_results['test_f1_macro'].mean().round(2))
    print("\n")

lr_model = LogisticRegression(random_state=17)

scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']
cv_results = cross_validate(lr_model, X, y, cv=5, scoring=scoring)

print("Average Accuracy:", cv_results['test_accuracy'].mean().round(2))
print("Average Precision:", cv_results['test_precision_macro'].mean().round(2))
print("Average Recall:", cv_results['test_recall_macro'].mean().round(2))
print("Average F1-score:", cv_results['test_f1_macro'].mean().round(2))

rf_model = RandomForestClassifier(random_state=17)

scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']
cv_results = cross_validate(rf_model, X, y, cv=5, scoring=scoring)

print("Average Accuracy:", cv_results['test_accuracy'].mean().round(2))
print("Average Precision:", cv_results['test_precision_macro'].mean().round(2))
print("Average Recall:", cv_results['test_recall_macro'].mean().round(2))
print("Average F1-score:", cv_results['test_f1_macro'].mean().round(2))

rf_model.get_params()

rf_params = {"max_depth": [5, 8, None], # depth of the decision trees
             "max_features": [3, 5, 7, 9], # number of features to consider when looking for the best split
             "min_samples_split": [2, 5, 8, 15, 20], # minimum number of samples required to split an internal node
             "n_estimators": [100, 200, 500] # number of trees in the random forest
             }

rf_best_grid = GridSearchCV(rf_model, rf_params, cv=5, n_jobs=-1, verbose=True).fit(X, y)

rf_best_grid.best_params_

rf_final = rf_model.set_params(**rf_best_grid.best_params_, random_state=17).fit(X, y)

cv_results = cross_validate(rf_final, X, y, cv=5, scoring=scoring)

print("Average Accuracy:", cv_results['test_accuracy'].mean().round(2))
print("Average Precision:", cv_results['test_precision_macro'].mean().round(2))
print("Average Recall:", cv_results['test_recall_macro'].mean().round(2))
print("Average F1-score:", cv_results['test_f1_macro'].mean().round(2))

def plot_importance(model, features, num=len(X), save=False):
    feature_imp = pd.DataFrame({"Value":model.feature_importances_, "Feature": features.columns})
    plt.figure(figsize=(10,10))
    sns.set(font_scale=1)
    sns.barplot(x="Value", y="Feature", data=feature_imp.sort_values(by="Value", ascending=False)[0:num])

    plt.title("Features")
    plt.tight_layout()
    plt.show()
    if save:
        plt.savefig("importance.png")

plot_importance(rf_model, X, save=True)

